\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023} 


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
  \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{array,multirow} % for table
\usepackage{diagbox}        % for table

\title{Homework 1 -- Graybox Attack}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.
\newcommand {\model}{
  \begin{tabular}{c}
    $resnet20\_cifar100$ \\
    $wrn16\_10\_cifar100$ \\
    $ror3\_164\_cifar100$ \\
    $rir\_cifar100$ \\
    $pyramidnet110\_a270\_cifar100$
  \end{tabular}
}

\newcommand {\defenseAccuracy}[2] {
  \begin{tabular}{c}
    #1 $\rightarrow$ #2 \\
  \end{tabular}
}

\newcommand {\accuracy}[5]{
  \begin{tabular}{c}
    #1 \\
    #2 \\
    #3 \\
    #4 \\
    #5 \\
  \end{tabular}
}

\newcommand {\DIFGSM} {
  M-DI$^{2}$-FGSM
}

\author{%
  Walker \\
  Department of Electrical Engineering \\
  B10901036 \\
}


\begin{document}


\maketitle


\begin{abstract}
  This homework is about implementing graybox attack.
  By using different kinds of attacks, such as FGSM, MI-FGSM, \DIFGSM and PGD, we can generate adversarial examples to attack model(s).
  Also, The transferbility of different adversarial attacks are also important.
  Moreover, different hyperparameters will affect the transferability.
  Last but not least, adversarial training will be implemented to see its effect on the model's robustness.
\end{abstract}

\section{Attack}
In this homework, I use CIFAR-100 dataset, which consists of 500 inference images in 100 classes, with 5 images per class.
I have tried various attacks on the CIFAR-100 dataset, including FGSM, MI-FGSM, \DIFGSM and PGD attack.
For each attack, I will check the model accuracy and transferbility of these adversarial examples.
To do so, I generate the adversarial examples in two ways:

\begin{enumerate}
    \item Single model attack
    \item Ensemble attack
\end{enumerate}

\subsection{Four Attack Methods}
I implement four different attacks on the CIFAR-100 dataset to generate adversarial examples.:

\begin{enumerate}
  \item Fast Gradient Sign Method (FGSM)
  \item Momentum Iterative Fast Gradient Sign Method (MIFGSM)
  \item Momentum Diverse Inputs Iterative Fast Gradient Sign Method (\DIFGSM)
  \item Projected Gradient Descent (PGD)
\end{enumerate}

There are plenty of models we can choose to attack from the model lists\footnote{The available models are listed in the github \url{https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/model_provider.py} }
I use $resnet20\_cifar100$ to generate the adversarial examples for single model attack,
and use four different models below to generate the adversarial examples for ensemble attack.

\begin{enumerate}
  \item $resnet20\_cifar100$
  \item $nin\_cifar100$
  \item $seresnet20\_cifar100$
  \item $densenet40\_k12\_cifar100$
\end{enumerate}

For ensemble attack, four models above will generate the adversarial examples respectively and then return the average image of the three adveresarial examples.

To check the relationship between the effect of adversarial attack and the transferbility of the adersarial examples, 
I use the generated adversarial examples above on 5 different models:

\begin{enumerate}
  \item $resnet20\_cifar100$
  \item $wrn16\_10\_cifar100$
  \item $ror3\_164\_cifar100$
  \item $rir\_cifar100$
  \item $pyramidnet110\_a270\_cifar100$
\end{enumerate}

The first model is the same as the model used to generate the adversarial examples (both single model attack and ensemble attack),
The other four models are used to test the transferbility of the adversarial examples.

For all of the attacks, the perturbation is limited by $\ell_\infty$-norm, with $\epsilon$ set to $\frac{8}{255}$.
For MI-FGSM, \DIFGSM and PGD attacks, the iteration number is set to $4$.
The results of the attacks are shown in Table \ref{table:attack_result}.

\begin{table}
  \centering
  \label{table:attack_result}
    \begin{tabular}{|c|c|c|c|}
      \hline
      Attack type & Attack method & Tested Model & Acc.\\
      \hline
      No Attack & Original & \model & \accuracy{0.496}{0.978}{0.862}{0.952}{0.986} \\
      \hline
      \hline
      \multirow{12}{*}{Single Model Attack} 
        & FGSM     & \model & \accuracy{0.140}{0.346}{0.208}{0.318}{0.474} \\
      \cline{2-4}
        & MI-FGSM   & \model & \accuracy{\textbf{0.000}}{0.274}{0.130}{0.218}{0.512} \\
      \cline{2-4}
        & \DIFGSM   & \model & \accuracy{0.002}{0.330}{0.138}{0.228}{0.546} \\
      \cline{2-4}
        & PGD      & \model & \accuracy{\textbf{0.000}}{0.449}{0.234}{0.410}{0.680} \\
      \hline
      \hline
      \multirow{12}{*}{Ensemble Attack} 
        & FGSM     & \model & \accuracy{0.044}{0.174}{0.096}{0.156}{0.298} \\
      \cline{2-4}
        & MI-FGSM   & \model & \accuracy{0.002}{\textbf{0.058}}{\textbf{0.048}}{\textbf{0.070}}{\textbf{0.020}} \\
      \cline{2-4}
        & \DIFGSM   & \model & \accuracy{0.010}{0.142}{0.092}{0.112}{0.292} \\
      \cline{2-4}
        & PGD      & \model & \accuracy{0.002}{0.130}{0.072}{0.124}{0.336} \\
      \hline
    \end{tabular}
  \newline
  \caption{The table shows the accuracy of the adversarial examples on the tested models with different attacks. 
          The \textbf{bold black number} represents the best attack (lowest accuracy) on the same model for all kinds of attack.}
\end{table}

We can see that MI-FGSM attack can not only generate the best adversarial examples for the same model attack,
but the examples can also transfer to other models with low accuracy.
Also, \textbf{ensemble attack can have better transferability than single model attack.}

\subsection{Hyperparameters}

For MI-FGSM, \DIFGSM and PGD attack, I have tried different iteration number to see whether the model accuracy and transferability will change.
The result is shown in Table \ref{table:hyperparameter_result}.

We can see that for MI-FGSM, \DIFGSM and PGD attack,
The more iteration number, the lower the model accuracy and the better the transferability on the tested models.
Also, Table \ref{table:hyperparameter_result} proves again that MI-FGSM attack can generate better adversarial examples with the lower accuracy on tested models compared with the other 2 attacks.
\begin{table}
  \centering
  \label{table:hyperparameter_result}
    \begin{tabular}{|c|c|c|c|}
      \hline
      Attack method & Tested Model & Small Iter Acc & Big Iter Acc\\
      \hline
      \multirow{1}{*}{MI-FGSM Attack} 
        & \model & \accuracy{\textbf{0.000}}{0.036}{0.014}{0.020}{0.132} & \accuracy{\textbf{0.000}}{\textbf{0.020}}{\textbf{0.008}}{\textbf{0.012}}{\textbf{0.110}}\\
      \hline
      \multirow{1}{*}{\DIFGSM Attack} 
        & \model & \accuracy{0.01}{0.142}{0.092}{0.112}{0.292} & \accuracy{\textbf{0.002}}{\textbf{0.052}}{\textbf{0.038}}{\textbf{0.056}}{\textbf{0.168}}\\
      \hline
      \multirow{1}{*}{PGD Attack} 
        & \model & \accuracy{\textbf{0.002}}{0.130}{0.052}{0.108}{\textbf{0.326}} & \accuracy{\textbf{0.002}}{\textbf{0.112}}{\textbf{0.048}}{\textbf{0.104}}{0.328} \\
      \hline
    \end{tabular}
  \newline
  \caption{The table shows the accuracy of the adversarial examples on the tested models with different iteration numbers for MI-FGSM, \DIFGSM and PGD attack.
          For all three attacks, the iteration numbers are all 16 and 64.
          The \textbf{bold black number} represents the better attack (lower accuracy) on the same model for the same attack.}
\end{table}


\section{Defense}

To see the effect of adversarial training, I use the adversarial examples generated from the attack mentioned above to train the model.

There are a total of 5000 images, with 3\footnote{no \DIFGSM examples since this attack method is implemented at last. Therefore, there is no enough time to re-test this experiment! } types of adversarial images, generated from

\begin{enumerate}
  \item FGSM attack $\cdot$ 1000
  \item MI-FGSM attack $\cdot$ 2000
  \item PGD attack $\cdot$ 2000
\end{enumerate}

These adversarial images are used to train the model with adversarial training respectively.

The model I choose for adversarial training is $resnet110\_cifar100$ . 
To see whether adversarial training with one type of adveresarial images can improve the model's robustness on other adversarial examples,
I train the model with three types of adversarial images respectively and test the model on the adversarial images.
The result is shown in Table \ref{table:defense_result}.

This result shows that for each training set of adversarial images, the accuracy tested on the same type of adversarial examples can have the best accuracy improvement.
Also, FGSM-based adversarial exmaples are better to defend since for both MI-FGSM-trained and PGD-trained models,
the accuracy improvements on FGSM adversarial examples enhance quite a lot (about 28 percent) compared with the other two types of adversarial examples (about 14 percent).
This can be an indirect sign that FGSM-based adversarial examples are weaker than the other two types of adversarial examples.


\begin{table}
  \centering
  \label{table:defense_result}
    \begin{tabular}{|c|c|c|c|}
      \hline
      \diagbox[width=12em, height=3.7em]{Training \\adversarial examples}{testing adversarial\\examples} & FGSM & MI-FGSM & PGD \\
      \hline
      FGSM & \textbf{\defenseAccuracy{0.187}{0.767}} & \defenseAccuracy{0.014}{0.058} & \defenseAccuracy{0.019}{0.049} \\
      MI-FGSM & \defenseAccuracy{0.016}{0.286} & \textbf{\defenseAccuracy{0.53}{0.953}} & \defenseAccuracy{0.006}{0.145} \\
      PGD & \defenseAccuracy{0.027}{0.294} & \defenseAccuracy{0.011}{0.147} & \textbf{\defenseAccuracy{0.073}{0.977}} \\
      \hline
    \end{tabular}
  \newline
  \caption{The table shows the accuracy before and after the adversarial training for different training and testing adversarial examples. 
          The \textbf{bold black accuracy change} represents the best accuracy improvement in each row.}
\end{table}

\section{Conclusion}

In this homework, I have implemented four different attacks on the CIFAR-100 dataset to generate adversarial examples.
I have also tried different hyperparameters for each attack to see the effect of the model accuracy and transferability of the adversarial examples.
The result shows that MI-FGSM attack can generate the best adversarial examples for the same model attack, and the examples can also transfer to the other model with the lowest accuracy.
Also, ensemble attack can have better transferability than single model attack.

I have also tried adversarial training to see the effect of adversarial training on the model's robustness.
The result shows that for each training set of adversarial images, the accuracy tested on the same type of adversarial examples can have the best accuracy improvement.
Also, FGSM-based adversarial exmaples are better to defend.
\section*{Homework Submission}

For submission, I use the adversarial examples generated from MI-FGSM attack with ensemble attack.
The iteration number is set to 16, and the model used to generate the adversarial examples are the same in sub-section 1.1.

\section*{Github Code}

The code for this homework is available on my github repository: \\
\url{https://github.com/walkerhsu/SPML/tree/main/greybox_attack}

\section*{References}



{
\small


[1] torchattack github link: {\url{https://github.com/Harry24k/adversarial-attacks-pytorch}}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}